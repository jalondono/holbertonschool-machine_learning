<article class=""><div id="jigsaw-shortcut-lists"></div><h1 class="gap">0x01. Deep Q-learning</h1><div id="project_id" style="display: none" data-project-id="784"></div><p class="sm-gap"><small><i class="fa fa-folder-open"></i> Specializations - Machine Learning ― Reinforcement Learning </small></p><p><em><small><i class="fa fa-user"></i> by Alexa Orrico, Software Engineer at Holberton School </small></em></p><p><small><i class="fa fa-calendar"></i> Ongoing project - started 10-07-2020, must end by 10-17-2020 (in 3 days) - you're done with <span id="student_task_done_percentage">0</span>% of tasks. </small></p><p><small><i class="fa fa-check-square"></i><strong>Manual QA review must be done</strong> (request it when you are done with the project) </small></p><article id="description" class="gap formatted-content"><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/8/9239a27ccd609cb9092aba0e6bb55ba7b5cf0b6b.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20201014%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20201014T032016Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=7a5fafc7717005176ef6ad1911c977fabaf6c721866fa5ffd445e136a5685b38" alt="" style=""></p><h2>Resources</h2><p><strong>Read or watch</strong>:</p><ul><li><a href="/rltoken/vf8M2yFL9vWcFftBWFG2KQ" title="Deep Q-Learning - Combining Neural Networks and Reinforcement Learning" target="_blank">Deep Q-Learning - Combining Neural Networks and Reinforcement Learning</a></li><li><a href="/rltoken/LciKBr548xY_iD4QkUatNw" title="Replay Memory Explained - Experience for Deep Q-Network Training" target="_blank">Replay Memory Explained - Experience for Deep Q-Network Training</a></li><li><a href="/rltoken/ZwReaNdr4Ei4GxWr-56oFg" title="Training a Deep Q-Network - Reinforcement Learning" target="_blank">Training a Deep Q-Network - Reinforcement Learning</a></li><li><a href="/rltoken/xAP3VzSnw0HLwjrBRn46Xw" title="Training a Deep Q-Network with Fixed Q-targets - Reinforcement Learning" target="_blank">Training a Deep Q-Network with Fixed Q-targets - Reinforcement Learning</a></li></ul><p><strong>References</strong>:</p><ul><li><a href="/rltoken/mSQhyiu7FEaFi_qTft1G2w" title="keras-rl" target="_blank">keras-rl</a><ul><li><a href="https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py" title="rl.policy" target="_blank">rl.policy</a></li><li><a href="https://github.com/keras-rl/keras-rl/blob/master/rl/memory.py" title="rl.memory" target="_blank">rl.memory</a></li><li><a href="https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py" title="rl.agents.dqn" target="_blank">rl.agents.dqn</a></li></ul></li><li><a href="/rltoken/SekcqEIbg0hxdEvoQSB-kA" title="Playing Atari with Deep Reinforcement Learning" target="_blank">Playing Atari with Deep Reinforcement Learning</a></li></ul><h2>Learning Objectives</h2><ul><li>What is Deep Q-learning?</li><li>What is the policy network?</li><li>What is replay memory?</li><li>What is the target network?</li><li>Why must we utilize two separate networks during training?</li><li>What is keras-rl? How do you use it?</li></ul><h2>Requirements</h2><h3>General</h3><ul><li>Allowed editors: <code>vi</code>, <code>vim</code>, <code>emacs</code></li><li>All your files will be interpreted/compiled on Ubuntu 16.04 LTS using <code>python3</code> (version 3.5)</li><li>Your files will be executed with <code>numpy</code> (version 1.15), <code>gym</code> (version 0.17.2), <code>keras</code> (version 2.2.5), and <code>keras-rl</code> (version 0.4.2)</li><li>All your files should end with a new line</li><li>The first line of all your files should be exactly <code>#!/usr/bin/env python3</code></li><li>A <code>README.md</code> file, at the root of the folder of the project, is mandatory</li><li>Your code should use the <code>pycodestyle</code> style (version 2.4)</li><li>All your modules should have documentation (<code>python3 -c 'print(__import__("my_module").__doc__)'</code>)</li><li>All your classes should have documentation (<code>python3 -c 'print(__import__("my_module").MyClass.__doc__)'</code>)</li><li>All your functions (inside and outside a class) should have documentation (<code>python3 -c 'print(__import__("my_module").my_function.__doc__)'</code> and <code>python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)'</code>)</li><li>All your files must be executable</li><li><strong>Your code should use the minimum number of operations</strong></li></ul><h2>Installing Keras-RL</h2><precode language="" precodenum="0"></precode><h3>Dependencies (that should already be installed)</h3><precode language="" precodenum="1"></precode></article><!-- Servers --><!-- Tasks --><hr class="gap"><h2 class="gap">Tasks</h2><section class="formatted-content"><div data-role="task6254" data-position="1"><div class=" clearfix gap" id="task-6254"><span id="user_id" data-id="870"></span><div class="student_task_controls"><!-- button Done --><button class="student_task_done btn btn-default no" data-task-id="6254"><span class="no"><i class="fa fa-square-o"></i></span><span class="yes"><i class="fa fa-check-square-o"></i></span><span class="pending"><i class="fa fa-spinner fa-pulse"></i></span> Done<span class="no pending">?</span><span class="yes">!</span></button><br><!-- button Help! --><button class="users_done_for_task btn btn-default btn-default" data-task-id="6254" data-project-id="784" data-toggle="modal" data-target="#task-6254-users-done-modal"> Help </button><div class="modal fade users-done-modal" id="task-6254-users-done-modal" data-task-id="6254" data-project-id="784"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h4 class="modal-title">Students who are done with "0. Breakout"</h4></div><div class="modal-body"><div class="list-group"></div><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="error"></div></div></div></div></div></div><h4 class="task"> 0. Breakout <span class="alert alert-warning mandatory-optional"> mandatory </span></h4><!-- Progress vs Score --><!-- Task Body --><p>Write a python script <code>train.py</code> that utilizes <code>keras</code>, <code>keras-rl</code>, and <code>gym</code> to train an agent that can play Atari’s Breakout:</p><ul><li>Your script should utilize <code>keras-rl</code>‘s <code>DQNAgent</code>, <code>SequentialMemory</code>, and <code>EpsGreedyQPolicy</code></li><li>Your script should save the final policy network as <code>policy.h5</code></li></ul><p>Write a python script <code>play.py</code> that can display a game played by the agent trained by <code>train.py</code>:</p><ul><li>Your script should load the policy network saved in <code>policy.h5</code></li><li>Your agent should use the <code>GreedyQPolicy</code></li></ul><!-- Task URLs --><!-- Github information --><p class="sm-gap"><strong>Repo:</strong></p><ul><li>GitHub repository: <code>holbertonschool-machine_learning</code></li><li>Directory: <code>reinforcement_learning/0x01-deep_q_learning</code></li><li>File: <code>train.py, play.py</code></li></ul></div></div></section></article>

