<article class=""><div id="jigsaw-shortcut-lists"></div><h1 class="gap">0x04. Autoencoders</h1><div id="project_id" style="display: none" data-project-id="544"></div><p class="sm-gap"><small><i class="fa fa-folder-open"></i> Specializations - Machine Learning ― Unsupervised Learning </small></p><p><em><small><i class="fa fa-user"></i> by Alexa Orrico, Software Engineer at Holberton School </small></em></p><p><small><i class="fa fa-calendar"></i> Ongoing project - started 08-24-2020, must end by 08-29-2020 (in 4 days) - you're done with <span id="student_task_done_percentage">0</span>% of tasks. </small></p><p><small><i class="fa fa-check"></i> Checker will be released at 08-26-2020 12:00 PM </small></p><p><small><i class="fa fa-check-square"></i> QA review fully automated. </small></p><article id="description" class="gap formatted-content"><p><strong>Read or watch</strong>:</p><ul><li><a href="/rltoken/WMbe5eWUHhyFet68Kf9pwQ" title="Autoencoder - definition" target="_blank">Autoencoder - definition</a></li><li><a href="/rltoken/9c-itgW_s8KR3osdRmmnKw" title="Autoencoder - loss function" target="_blank">Autoencoder - loss function</a></li><li><a href="/rltoken/3hm5-oOrajXejrCH4P1-Mg" title="Deep learning - deep autoencoder" target="_blank">Deep learning - deep autoencoder</a></li><li><a href="/rltoken/qoMN_ZeSetmF92dXhinEAA" title="Introduction to autoencoders" target="_blank">Introduction to autoencoders</a></li><li><a href="/rltoken/6TUoTdWuAyB4SGEw5Jf-Cg" title="Variational Autoencoders - EXPLAINED!" target="_blank">Variational Autoencoders - EXPLAINED!</a>¨NBSP;<em>up to</em><strong>12:55</strong></li><li><a href="/rltoken/VV_w6WaFyY8e0jF5qXmy4g" title="Variational Autoencoders" target="_blank">Variational Autoencoders</a></li><li><a href="/rltoken/ApWzU1MyKFmkTeqeDyuaiw" title="Intuitively Understanding Variational Autoencoders" target="_blank">Intuitively Understanding Variational Autoencoders</a></li><li><a href="/rltoken/N_A8AaQexJgGx9TYjDGpxA" title="Deep Generative Models" target="_blank">Deep Generative Models</a><em>up to</em><strong>Generative Adversarial Networks</strong></li></ul><p><strong>Definitions to skim</strong>:</p><ul><li><a href="/rltoken/M0-vm4JIoP-Msl5SemHDDA" title="Kullback–Leibler divergence" target="_blank">Kullback–Leibler divergence</a><em>recall its use in t-SNE</em></li><li><a href="/rltoken/kgDcseTs0_TQ5X7nPs7EeQ" title="Autoencoder" target="_blank">Autoencoder</a></li><li><a href="/rltoken/oICoCfZORJNQFwcpR-Kq1w" title="Generative model" target="_blank">Generative model</a></li></ul><p><strong>References</strong>:</p><ul><li><a href="/rltoken/lAQEyGkZx9q4vnXTD8noxQ" title="The Deep Learning textbook - Chapter 14: Autoencoders" target="_blank">The Deep Learning textbook - Chapter 14: Autoencoders</a></li><li><a href="/rltoken/EsJJMSqKlVcbBvjLEMhutQ" title="Reducing the Dimensionality of Data with Neural Networks 2006" target="_blank">Reducing the Dimensionality of Data with Neural Networks 2006</a></li></ul><h2>Learning Objectives</h2><ul><li>What is an autoencoder?</li><li>What is latent space?</li><li>What is a bottleneck?</li><li>What is a sparse autoencoder?</li><li>What is a convolutional autoencoder?</li><li>What is a generative model?</li><li>What is a variational autoencoder?</li><li>What is the Kullback-Leibler divergence?</li></ul><h2>Requirements</h2><h3>General</h3><ul><li>Allowed editors: <code>vi</code>, <code>vim</code>, <code>emacs</code></li><li>All your files will be interpreted/compiled on Ubuntu 16.04 LTS using <code>python3</code> (version 3.5)</li><li>Your files will be executed with <code>numpy</code> (version 1.15) and <code>tensorflow</code> (version 1.12)</li><li>All your files should end with a new line</li><li>The first line of all your files should be exactly <code>#!/usr/bin/env python3</code></li><li>A <code>README.md</code> file, at the root of the folder of the project, is mandatory</li><li>Your code should use the <code>pycodestyle</code> style (version 2.4)</li><li>All your modules should have documentation (<code>python3 -c 'print(__import__("my_module").__doc__)'</code>)</li><li>All your classes should have documentation (<code>python3 -c 'print(__import__("my_module").MyClass.__doc__)'</code>)</li><li>All your functions (inside and outside a class) should have documentation (<code>python3 -c 'print(__import__("my_module").my_function.__doc__)'</code> and <code>python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)'</code>)</li><li>Unless otherwise noted, you are not allowed to import any module except <code>import tensorflow.keras as keras</code></li><li>All your files must be executable</li></ul></article><hr class="gap"><h2 class="gap">Quiz questions</h2><p id="quiz_questions_collapse_toggle">Show</p><section class="formatted-content quiz_questions_show_container" style="display: none;"><div class="quiz_question_item_container" data-role="quiz_question1060" data-position="1"><div class=" clearfix" id="quiz_question-1060"><h4 class="quiz_question">Question #0</h4><!-- Quiz question tags --><!-- Quiz question Body --><p>What is a “vanilla” autoencoder?</p><!-- Quiz question Answers --><ul class="quiz_question_answers" data-question-id="1060"><li class=""><input type="checkbox" data-quiz-question-id="1060" data-quiz-answer-id="1592200552011" disabled="" checked=""><p>A compression model</p></li><li class=""><input type="checkbox" data-quiz-question-id="1060" data-quiz-answer-id="1592200553031" disabled="" checked=""><p>Composed of an encoder and decoder</p></li><li class=""><input type="checkbox" data-quiz-question-id="1060" data-quiz-answer-id="1592200554288" disabled=""><p>A generative model</p></li><li class=""><input type="checkbox" data-quiz-question-id="1060" data-quiz-answer-id="1592200555724" disabled="" checked=""><p>Learns a latent space representation</p></li></ul><!-- Quiz question Tips --></div></div><div class="quiz_question_item_container" data-role="quiz_question1061" data-position="2"><div class=" clearfix" id="quiz_question-1061"><h4 class="quiz_question">Question #1</h4><!-- Quiz question tags --><!-- Quiz question Body --><p>What is a bottleneck?</p><!-- Quiz question Answers --><ul class="quiz_question_answers" data-question-id="1061"><li class=""><input type="checkbox" data-quiz-question-id="1061" data-quiz-answer-id="1592200632028" disabled=""><p>When you can no longer train your model</p></li><li class=""><input type="checkbox" data-quiz-question-id="1061" data-quiz-answer-id="1592200633327" disabled=""><p>The latent space representation</p></li><li class=""><input type="checkbox" data-quiz-question-id="1061" data-quiz-answer-id="1592200634666" disabled=""><p>The compressed input</p></li><li class=""><input type="checkbox" data-quiz-question-id="1061" data-quiz-answer-id="1592200635964" disabled="" checked=""><p>A layer that is smaller than the previous and next layers</p></li></ul><!-- Quiz question Tips --></div></div><div class="quiz_question_item_container" data-role="quiz_question1062" data-position="3"><div class=" clearfix" id="quiz_question-1062"><h4 class="quiz_question">Question #2</h4><!-- Quiz question tags --><!-- Quiz question Body --><p>What is a VAE?</p><!-- Quiz question Answers --><ul class="quiz_question_answers" data-question-id="1062"><li class=""><input type="checkbox" data-quiz-question-id="1062" data-quiz-answer-id="1592200749082" disabled=""><p>An adversarial network</p></li><li class=""><input type="checkbox" data-quiz-question-id="1062" data-quiz-answer-id="1592200750045" disabled="" checked=""><p>A generative model</p></li><li class=""><input type="checkbox" data-quiz-question-id="1062" data-quiz-answer-id="1592200751435" disabled="" checked=""><p>Composed of an encoder and decoder</p></li><li class=""><input type="checkbox" data-quiz-question-id="1062" data-quiz-answer-id="1592200753049" disabled=""><p>A compression model</p></li></ul><!-- Quiz question Tips --></div></div><div class="quiz_question_item_container" data-role="quiz_question1063" data-position="4"><div class=" clearfix" id="quiz_question-1063"><h4 class="quiz_question">Question #3</h4><!-- Quiz question tags --><!-- Quiz question Body --><p>What loss function(s) is/are used for training <code>vanilla</code> autoencoders?</p><!-- Quiz question Answers --><ul class="quiz_question_answers" data-question-id="1063"><li class=""><input type="checkbox" data-quiz-question-id="1063" data-quiz-answer-id="1592200848286" disabled="" checked=""><p>Mean Squared Error</p></li><li class=""><input type="checkbox" data-quiz-question-id="1063" data-quiz-answer-id="1592200849545" disabled=""><p>L2 Normalization</p></li><li class=""><input type="checkbox" data-quiz-question-id="1063" data-quiz-answer-id="1592200851046" disabled="" checked=""><p>Cross Entropy</p></li><li class=""><input type="checkbox" data-quiz-question-id="1063" data-quiz-answer-id="1592200852446" disabled=""><p>Kullback-Leibler Divergence</p></li></ul><!-- Quiz question Tips --></div></div><div class="quiz_question_item_container" data-role="quiz_question1064" data-position="5"><div class=" clearfix" id="quiz_question-1064"><h4 class="quiz_question">Question #4</h4><!-- Quiz question tags --><!-- Quiz question Body --><p>What loss function(s) is/are used for training variational autoencoders?</p><!-- Quiz question Answers --><ul class="quiz_question_answers" data-question-id="1064"><li class=""><input type="checkbox" data-quiz-question-id="1064" data-quiz-answer-id="1592200889013" disabled="" checked=""><p>Mean Squared Error</p></li><li class=""><input type="checkbox" data-quiz-question-id="1064" data-quiz-answer-id="1592200890052" disabled=""><p>L2 Normalization</p></li><li class=""><input type="checkbox" data-quiz-question-id="1064" data-quiz-answer-id="1592200891411" disabled="" checked=""><p>Cross Entropy</p></li><li class=""><input type="checkbox" data-quiz-question-id="1064" data-quiz-answer-id="1592200892907" disabled="" checked=""><p>Kullback-Leibler Divergence</p></li></ul><!-- Quiz question Tips --></div></div></section><!-- Servers --><!-- Tasks --><hr class="gap"><h2 class="gap">Tasks</h2><section class="formatted-content"><div data-role="task4926" data-position="1"><div class=" clearfix gap" id="task-4926"><span id="user_id" data-id="870"></span><div class="student_task_controls"><!-- button Done --><button class="student_task_done btn btn-default no" data-task-id="4926"><span class="no"><i class="fa fa-square-o"></i></span><span class="yes"><i class="fa fa-check-square-o"></i></span><span class="pending"><i class="fa fa-spinner fa-pulse"></i></span> Done<span class="no pending">?</span><span class="yes">!</span></button><br><!-- button Help! --><button class="users_done_for_task btn btn-default btn-default" data-task-id="4926" data-project-id="544" data-toggle="modal" data-target="#task-4926-users-done-modal"> Help </button><div class="modal fade users-done-modal" id="task-4926-users-done-modal" data-task-id="4926" data-project-id="544"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h4 class="modal-title">Students who are done with "0. "Vanilla" Autoencoder"</h4></div><div class="modal-body"><div class="list-group"></div><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="error"></div></div></div></div></div></div><h4 class="task"> 0. "Vanilla" Autoencoder <span class="alert alert-warning mandatory-optional"> mandatory </span></h4><!-- Progress vs Score --><!-- Task Body --><p>Write a function <code>def autoencoder(input_dims, hidden_layers, latent_dims):</code> that creates an autoencoder:</p><ul><li><code>input_dims</code> is an integer containing the dimensions of the model input</li><li><code>hidden_layers</code> is a list containing the number of nodes for each hidden layer in the encoder, respectively <ul><li>the hidden layers should be reversed for the decoder</li></ul></li><li><code>latent_dims</code> is an integer containing the dimensions of the latent space representation</li><li>Returns: <code>encoder, decoder, auto</code><ul><li><code>encoder</code> is the encoder model</li><li><code>decoder</code> is the decoder model</li><li><code>auto</code> is the full autoencoder model</li></ul></li><li>The autoencoder model should be compiled using adam optimization and binary cross-entropy loss</li><li>All layers should use a <code>relu</code> activation except for the last layer in the decoder, which should use <code>sigmoid</code></li></ul><precode language="" precodenum="0"></precode><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/6/9c11026a690360f31ce76fd7e5e3515e7cda6925.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20200824%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200824T210308Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=8dac62ab9130cd557ce2baa4de7bba92b1c83097b2e47265189be13d141b7867" alt="" style=""></p><!-- Task URLs --><!-- Github information --><p class="sm-gap"><strong>Repo:</strong></p><ul><li>GitHub repository: <code>holbertonschool-machine_learning</code></li><li>Directory: <code>unsupervised_learning/0x04-autoencoders</code></li><li>File: <code>0-vanilla.py</code></li></ul></div></div><div data-role="task4927" data-position="2"><div class=" clearfix gap" id="task-4927"><span id="user_id" data-id="870"></span><div class="student_task_controls"><!-- button Done --><button class="student_task_done btn btn-default no" data-task-id="4927"><span class="no"><i class="fa fa-square-o"></i></span><span class="yes"><i class="fa fa-check-square-o"></i></span><span class="pending"><i class="fa fa-spinner fa-pulse"></i></span> Done<span class="no pending">?</span><span class="yes">!</span></button><br><!-- button Help! --><button class="users_done_for_task btn btn-default btn-default" data-task-id="4927" data-project-id="544" data-toggle="modal" data-target="#task-4927-users-done-modal"> Help </button><div class="modal fade users-done-modal" id="task-4927-users-done-modal" data-task-id="4927" data-project-id="544"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h4 class="modal-title">Students who are done with "1. Sparse Autoencoder"</h4></div><div class="modal-body"><div class="list-group"></div><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="error"></div></div></div></div></div></div><h4 class="task"> 1. Sparse Autoencoder <span class="alert alert-warning mandatory-optional"> mandatory </span></h4><!-- Progress vs Score --><!-- Task Body --><p>Write a function <code>def sparse(input_dims, hidden_layers, latent_dims, lambtha):</code> that creates a sparse autoencoder:</p><ul><li><code>input_dims</code> is an integer containing the dimensions of the model input</li><li><code>hidden_layers</code> is a list containing the number of nodes for each hidden layer in the encoder, respectively <ul><li>the hidden layers should be reversed for the decoder</li></ul></li><li><code>latent_dims</code> is an integer containing the dimensions of the latent space representation</li><li><code>lambtha</code> is the regularization parameter used for L1 regularization on the encoded output</li><li>Returns: <code>encoder, decoder, auto</code><ul><li><code>encoder</code> is the encoder model</li><li><code>decoder</code> is the decoder model</li><li><code>auto</code> is the sparse autoencoder model</li></ul></li><li>The sparse autoencoder model should be compiled using adam optimization and binary cross-entropy loss</li><li>All layers should use a <code>relu</code> activation except for the last layer in the decoder, which should use <code>sigmoid</code></li></ul><precode language="" precodenum="1"></precode><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/6/4259218c39ff1a590ed499023c84b5a5242b91c2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20200824%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200824T210308Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=6620555a33e5f1bf4454c1b579cc37599586e6cb43b2cb23b1893a4d05aade04" alt="" style=""></p><!-- Task URLs --><!-- Github information --><p class="sm-gap"><strong>Repo:</strong></p><ul><li>GitHub repository: <code>holbertonschool-machine_learning</code></li><li>Directory: <code>unsupervised_learning/0x04-autoencoders</code></li><li>File: <code>1-sparse.py</code></li></ul></div></div><div data-role="task4928" data-position="3"><div class=" clearfix gap" id="task-4928"><span id="user_id" data-id="870"></span><div class="student_task_controls"><!-- button Done --><button class="student_task_done btn btn-default no" data-task-id="4928"><span class="no"><i class="fa fa-square-o"></i></span><span class="yes"><i class="fa fa-check-square-o"></i></span><span class="pending"><i class="fa fa-spinner fa-pulse"></i></span> Done<span class="no pending">?</span><span class="yes">!</span></button><br><!-- button Help! --><button class="users_done_for_task btn btn-default btn-default" data-task-id="4928" data-project-id="544" data-toggle="modal" data-target="#task-4928-users-done-modal"> Help </button><div class="modal fade users-done-modal" id="task-4928-users-done-modal" data-task-id="4928" data-project-id="544"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h4 class="modal-title">Students who are done with "2. Convolutional Autoencoder"</h4></div><div class="modal-body"><div class="list-group"></div><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="error"></div></div></div></div></div></div><h4 class="task"> 2. Convolutional Autoencoder <span class="alert alert-warning mandatory-optional"> mandatory </span></h4><!-- Progress vs Score --><!-- Task Body --><p>Write a function <code>def autoencoder(input_dims, filters, latent_dims):</code> that creates a convolutional autoencoder:</p><ul><li><code>input_dims</code> is a tuple of integers containing the dimensions of the model input</li><li><code>filters</code> is a list containing the number of filters for each convolutional layer in the encoder, respectively <ul><li>the filters should be reversed for the decoder</li></ul></li><li><code>latent_dims</code> is a tuple of integers containing the dimensions of the latent space representation</li><li>Each convolution in the encoder should use a kernel size of <code>(3, 3)</code> with same padding and <code>relu</code> activation, followed by max pooling of size <code>(2, 2)</code></li><li>Each convolution in the decoder, except for the last two, should use a filter size of <code>(3, 3)</code> with same padding and <code>relu</code> activation, followed by upsampling of size <code>(2, 2)</code><ul><li>The second to last convolution should instead use valid padding</li><li>The last convolution should have the same number of filters as the number of channels in <code>input_dims</code> with <code>sigmoid</code> activation and no upsampling</li></ul></li><li>Returns: <code>encoder, decoder, auto</code><ul><li><code>encoder</code> is the encoder model</li><li><code>decoder</code> is the decoder model</li><li><code>auto</code> is the full autoencoder model</li></ul></li><li>The autoencoder model should be compiled using adam optimization and binary cross-entropy loss</li></ul><precode language="" precodenum="2"></precode><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/6/9ae05866da7bb93a051bd59028bc0885d14ea71c.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20200824%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200824T210308Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=86c16c7538bc73f5ab684e6df4fe0b5106f76c37f440800cbfd3e2cef74e1742" alt="" style=""></p><!-- Task URLs --><!-- Github information --><p class="sm-gap"><strong>Repo:</strong></p><ul><li>GitHub repository: <code>holbertonschool-machine_learning</code></li><li>Directory: <code>unsupervised_learning/0x04-autoencoders</code></li><li>File: <code>2-convolutional.py</code></li></ul></div></div><div data-role="task4929" data-position="4"><div class=" clearfix gap" id="task-4929"><span id="user_id" data-id="870"></span><div class="student_task_controls"><!-- button Done --><button class="student_task_done btn btn-default no" data-task-id="4929"><span class="no"><i class="fa fa-square-o"></i></span><span class="yes"><i class="fa fa-check-square-o"></i></span><span class="pending"><i class="fa fa-spinner fa-pulse"></i></span> Done<span class="no pending">?</span><span class="yes">!</span></button><br><!-- button Help! --><button class="users_done_for_task btn btn-default btn-default" data-task-id="4929" data-project-id="544" data-toggle="modal" data-target="#task-4929-users-done-modal"> Help </button><div class="modal fade users-done-modal" id="task-4929-users-done-modal" data-task-id="4929" data-project-id="544"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h4 class="modal-title">Students who are done with "3. Variational Autoencoder"</h4></div><div class="modal-body"><div class="list-group"></div><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="error"></div></div></div></div></div></div><h4 class="task"> 3. Variational Autoencoder <span class="alert alert-warning mandatory-optional"> mandatory </span></h4><!-- Progress vs Score --><!-- Task Body --><p>Write a function <code>def autoencoder(input_dims, hidden_layers, latent_dims):</code> that creates a variational autoencoder:</p><ul><li><code>input_dims</code> is an integer containing the dimensions of the model input</li><li><code>hidden_layers</code> is a list containing the number of nodes for each hidden layer in the encoder, respectively <ul><li>the hidden layers should be reversed for the decoder</li></ul></li><li><code>latent_dims</code> is an integer containing the dimensions of the latent space representation</li><li>Returns: <code>encoder, decoder, auto</code><ul><li><code>encoder</code> is the encoder model, which should output the latent representation, the mean, and the log variance, respectively</li><li><code>decoder</code> is the decoder model</li><li><code>auto</code> is the full autoencoder model</li></ul></li><li>The autoencoder model should be compiled using adam optimization and binary cross-entropy loss</li><li>All layers should use a <code>relu</code> activation except for the mean and log variance layers in the encoder, which should use <code>None</code>, and the last layer in the decoder, which should use <code>sigmoid</code></li></ul><precode language="" precodenum="3"></precode><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/6/746ae5925f1f5dd60ebd3e5e2df45bfeb955f939.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20200824%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200824T210308Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=a8ffbd3f6d20eeb454fe4aa4c1973f203870a950a51f54ea1a9b564e6e43ee5c" alt="" style=""></p><p><img src="https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/6/aa86806adfdc4c2ea7394df066867c47d0beaf93.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARDDGGGOUWMNL5ANN%2F20200824%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20200824T210308Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=cfa7852957eaa2ce807accd7799b46fab5c95c6369ce638ac0a747321d948a99" alt="" style=""></p><!-- Task URLs --><!-- Github information --><p class="sm-gap"><strong>Repo:</strong></p><ul><li>GitHub repository: <code>holbertonschool-machine_learning</code></li><li>Directory: <code>unsupervised_learning/0x04-autoencoders</code></li><li>File: <code>3-variational.py</code></li></ul></div></div></section></article>

